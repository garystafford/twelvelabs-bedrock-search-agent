import json
import logging
import os
import time
import warnings

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError
from dotenv import load_dotenv
from opensearchpy import OpenSearch
from strands import tool
from twelvelabs import TwelveLabs

from data import (
    VideoSearchResult,
    VideoSearchResults,
    VideoSegmentSearchResult,
    VideoSegmentSearchResults,
)
from gradio_logger import GradioLogger

# Load environment variables from .env file
load_dotenv()

# Sensitive environment variables
AWS_REGION_MARENGO = os.getenv("AWS_REGION_MARENGO", "us-east-1")
S3_VIDEO_STORAGE_BUCKET_MARENGO = os.getenv("S3_VIDEO_STORAGE_BUCKET_MARENGO")

OPENSEARCH_ENDPOINT = os.getenv("OPENSEARCH_ENDPOINT")
OPENSEARCH_INDEX_NAME = os.getenv("OPENSEARCH_INDEX_NAME")

# Optional if using TwelveLabs SaaS for embeddings
TL_API_KEY = os.getenv("TL_API_KEY")

# Amazon Bedrock model ID
MODEL_ID_MARENGO = "twelvelabs.marengo-embed-2-7-v1:0"

# Embeddings output location on S3
S3_DESTINATION_PREFIX = "embeddings"


class CustomTools:
    """A collection of tools for interacting with AWS services and performing operations."""

    def __init__(self, logger: logging.Logger):
        gradio_logger = GradioLogger()
        self.logger = gradio_logger.setup_logging()

        # S3 clients for different regions/models
        self.s3_client_us_east_1 = boto3.client("s3", region_name=AWS_REGION_MARENGO)

        # Bedrock runtime client with retry configuration
        config = Config(
            retries={
                "max_attempts": 5,
                "mode": "standard",  # Or 'adaptive' for a more sophisticated approach
            }
        )

        self.bedrock_runtime_client = boto3.client(
            service_name="bedrock-runtime",
            region_name=AWS_REGION_MARENGO,
            config=config,
        )

        # This will hold the embedding generated by the Marengo model
        self.text_embedding: list[float] = []

    def generate_text_embedding_twelvelabs_saas(self, search_text) -> list[float]:
        """Convert a text query to an embedding using TwelveLabs SaaS.
        This function uses the TwelveLabs Marengo model to generate a text embedding.
        It sends the text query to the TwelveLabs API and retrieves the embedding.

        Args:
            query (str): The text query to convert.

        Returns:
            list[float]: The embedding vector.
        """
        if TL_API_KEY is None:
            raise ValueError("TL_API_KEY environment variable is not set")
        tl_client = TwelveLabs(api_key=TL_API_KEY)

        response = tl_client.embed.create(
            model_name="Marengo-retrieval-2.7",
            text_truncate="start",
            text=search_text,
        )

        if (
            response.text_embedding is not None
            and response.text_embedding.segments is not None
            and response.text_embedding.segments[0].embeddings_float is not None
        ):
            text_embedding = response.text_embedding.segments[0].embeddings_float
            self.logger.info(
                f"Generated text embedding for query: {search_text[:50]}... with length: {len(text_embedding)}"
            )
            return text_embedding
        else:
            raise ValueError("Failed to retrieve embedding from the response.")

    def generate_text_embedding_bedrock(self, search_text) -> dict:
        """Generates a text embedding using the Marengo model.
        Args:
            search_text (str): The search query text.
        Returns:
            dict: The response from the video analysis job.
        """
        try:
            response = self.bedrock_runtime_client.start_async_invoke(
                modelId=MODEL_ID_MARENGO,
                modelInput={
                    "inputType": "text",
                    "inputText": search_text,
                },
                outputDataConfig={
                    "s3OutputDataConfig": {
                        "s3Uri": f"s3://{S3_VIDEO_STORAGE_BUCKET_MARENGO}/{S3_DESTINATION_PREFIX}/",
                    }
                },
            )
            return response
        except ClientError as err:
            self.logger.error(f"Failed to generate text embedding: {err}")
            raise err

    def download_search_embedding_from_s3(self, s3_key: str) -> dict:
        """Download the output file from S3 and save it locally.
        Args:
            s3_key (str): The S3 key of the output file.
        Returns:
            VideoEmbeddings: The video embedding object.
        """
        try:
            s3_object = self.s3_client_us_east_1.get_object(
                Bucket=S3_VIDEO_STORAGE_BUCKET_MARENGO,
                Key=s3_key,
            )
            embedding = json.loads(s3_object["Body"].read().decode("utf-8"))
            return embedding
        except ClientError as err:
            self.logger.error(f"Failed to download text embedding from S3: {err}")
            raise err

    def poll_job_status(self, invocation_arn: str) -> str:
        """Poll the job status until it is completed or failed.
        Args:
            invocation_arn (str): The ARN of the job invocation.
        Returns:
            str: The final job status.
        """
        try:
            while True:
                response = self.bedrock_runtime_client.get_async_invoke(
                    invocationArn=invocation_arn
                )
                status = response["status"]

                self.logger.info(f"Invocation status: {status}")

                if status == "Completed":
                    self.logger.info("Job completed!")
                    break
                elif status == "Failed":
                    self.logger.info(f"Job failed: {response.get('failureMessage')}")
                    break
                else:
                    # Still in progress, so wait and retry
                    time.sleep(0.5)  # Adjust polling interval as necessary

            return response["status"]
        except ClientError as err:
            self.logger.error(f"Failed to poll job status: {err}")
            raise err

    @tool
    def create_text_embedding(self, search_text: str):
        """Creates a text embedding using the TwelveLabs Marengo model on Amazon Bedrock.
        Args:
            search_text (str): The text to be embedded.
        Raises:
            ValueError: If the embedding is not found in the response.
            botocore.exceptions.ClientError: If the job fails or the S3 download fails.
        """
        # Generate embeddings for the search text using Amazon Bedrock
        self.logger.info(
            f'Generating text embedding using Amazon Bedrock for: "{search_text}"'
        )
        response = self.generate_text_embedding_bedrock(search_text)
        invocation_arn = response["invocationArn"]
        self.logger.info(f"Invocation ARN: {invocation_arn}")

        # Poll the job status until it is completed
        response = self.poll_job_status(invocation_arn)
        self.logger.info(f"Job completed with status: {response}")

        # Download the output.json file from S3
        s3_key = f"{S3_DESTINATION_PREFIX}/{invocation_arn.split('/')[-1]}/output.json"
        self.logger.info(f"Downloading embedding from S3 key: {s3_key}")
        text_embedding = self.download_search_embedding_from_s3(s3_key)

        # Extract the text embedding from the response
        text_embedding = text_embedding["data"][0]["embedding"]
        self.logger.info(f"Text embedding: {text_embedding[0:5]}")
        self.text_embedding = text_embedding

        # # Generate embedding for the search text using TwelveLabs SaaS
        # self.logger.info(
        #     f'Generating text embedding using TwelveLabs SaaS for: "{search_text}"'
        # )
        # self.text_embedding = self.generate_text_embedding_twelvelabs_saas(search_text)

    def create_opensearch_client(self) -> OpenSearch:
        """Creates an OpenSearch client instance.
        Args:
            None
        Returns:
            OpenSearch: The OpenSearch client instance.
        """
        # Suppress security warnings related to unverified HTTPS requests and SSL connections
        warnings.filterwarnings("ignore", message="Unverified HTTPS request")
        warnings.filterwarnings(
            "ignore", message="Connecting to https://localhost:9200 using SSL"
        )

        os_client = OpenSearch(
            hosts=[{"host": OPENSEARCH_ENDPOINT, "port": 9200}],
            http_auth=("admin", "OpenSearch123"),
            use_ssl=True,
            verify_certs=False,
        )

        return os_client

    def semantic_search(
        self, opensearch_client: OpenSearch, text_embedding: list, results_size: int = 6
    ) -> dict:
        """Query the OpenSearch index using a text embedding and return a list of video search results.
        This function performs a semantic search in OpenSearch using the provided text embedding.
        It constructs a query that uses the k-nearest neighbors (kNN) algorithm to find
        the most similar video segments based on the embedding.
        The results are limited to the specified number of results_size.
        Args:
            opensearch_client (OpenSearch): The OpenSearch client instance.
            text_embedding (list): The text embedding to use for the search.
            results_size (int): The number of results to return.
        Returns:
            dict: The search results from OpenSearch.
        """
        query = {
            "query": {
                "nested": {
                    "path": "embeddings",
                    "query": {
                        "knn": {
                            "embeddings.embedding": {
                                "vector": text_embedding,
                                "k": results_size,
                            }
                        }
                    },
                }
            },
            "size": results_size,
            "_source": {"excludes": ["embeddings.embedding"]},
        }
        try:
            search_results = opensearch_client.search(
                body=query, index=OPENSEARCH_INDEX_NAME
            )
            self.logger.debug(f"Search results: {search_results}")
            return search_results
        except Exception as err:
            self.logger.error(f"Error querying index: {err}")
            raise err

    def keyword_search(
        self, opensearch_client: OpenSearch, keyword_list: list, results_size: int = 6
    ) -> dict:
        """Query the OpenSearch index using a text embedding and return a list of video search results.
        This function performs a keyword search in OpenSearch using the provided keyword list.
        Args:
            opensearch_client (OpenSearch): The OpenSearch client instance.
            keyword_list (list): The list of keywords to search for.
            results_size (int): The number of results to return.
        Returns:
            dict: The search results from OpenSearch.
        """
        query = {
            "query": {"terms": {"keywords": keyword_list}},
            "size": results_size,
            "_source": {"excludes": ["embeddings.embedding"]},
        }

        try:
            search_results = opensearch_client.search(
                body=query, index=OPENSEARCH_INDEX_NAME
            )
            self.logger.debug(f"Search results: {search_results}")
            return search_results
        except Exception as err:
            self.logger.error(f"Error querying index: {err}")
            raise err

    def format_search_results(self, raw_search_results: dict) -> dict:
        """Formats the raw search results for videos from OpenSearch into a structured format.
        This function processes the raw search results returned by OpenSearch and converts them
        into a structured format that includes video metadata such as video name, title, summary,
        keywords, duration, S3 URI, keyframe URL, and score.
        Args:
            raw_search_results (dict): The raw search results from OpenSearch.
        Returns:
            dict: The formatted search results.
        """
        search_results = VideoSearchResults(results=[])

        for result in raw_search_results["hits"]["hits"]:
            source = result["_source"]
            search_result = VideoSearchResult(
                videoName=source["videoName"],
                title=source["title"],
                summary=source["summary"],
                keywords=source["keywords"],
                durationSec=source["durationSec"],
                s3URI=source["s3URI"],
                keyframeURL=source["keyframeURL"],
                score=result["_score"],
            )

            search_results.results.append(search_result)

        return search_results.to_dict()

    @tool
    def semantic_search_for_videos(self, results_size: int = 6) -> dict:
        """Performs a semantic search for a list of unique videos using the generated text embedding.
        This function uses the text embedding generated by the Marengo model to search for videos in OpenSearch.
        Args:
            text_embedding (list): The dense vector embedding (list of floats) to use for the search.
            results_size (int): The number of results to return.
        Returns:
            dict: The search results from OpenSearch.
        """
        if len(self.text_embedding) != 1_024:
            self.logger.error(
                "Text embedding does not have the correct dimensions. Cannot perform search."
            )
            return {}
        self.logger.debug(f"Text embedding dimensions: {len(self.text_embedding)}")

        # Create an OpenSearch client
        opensearch_client = self.create_opensearch_client()

        # Perform the semantic search
        self.logger.info(
            f"Performing semantic search with embedding: {self.text_embedding[0:5]}..."
        )
        raw_search_results = self.semantic_search(
            opensearch_client, self.text_embedding, results_size
        )

        # Format the search results
        search_results = self.format_search_results(raw_search_results)
        self.logger.debug(f"Search results: {search_results}")
        return search_results

    @tool
    def keyword_search_for_videos(
        self, keyword_list: list, results_size: int = 6
    ) -> dict:
        """Performs a keyword search for a list of unique videos using the generated text embedding.
        This function uses the text embedding generated by the Marengo model to search for videos in OpenSearch.
        Args:
            text_embedding (list): The dense vector embedding (list of floats) to use for the search.
            results_size (int): The number of results to return.
        Returns:
            dict: The search results from OpenSearch.
        """
        if not keyword_list:
            self.logger.error("Keyword list is empty. Cannot perform search.")
            return {}
        self.logger.debug(f"Keyword list: {keyword_list}")

        # Create an OpenSearch client
        opensearch_client = self.create_opensearch_client()

        # Perform the keyword search
        self.logger.info(f"Performing keyword search with keywords: {keyword_list}...")
        raw_search_results = self.keyword_search(
            opensearch_client, keyword_list, results_size
        )

        # Format the search results
        search_results = self.format_search_results(raw_search_results)
        self.logger.debug(f"Search results: {search_results}")
        return search_results

    def semantic_search_segments(
        self, opensearch_client: OpenSearch, text_embedding: list, results_size: int = 6
    ) -> dict:
        """Performs a semantic search in OpenSearch using the provided text embedding and returns a list of video segments.
        This function constructs a query that uses the k-nearest neighbors (kNN) algorithm to find
        the most similar video segments based on the embedding. The results are limited to the specified number of results_size.
        Args:
            opensearch_client (OpenSearch): The OpenSearch client instance.
            text_embedding (list): The text embedding to use for the search.
            results_size (int): The number of results to return.
        Returns:
            dict: The search response from OpenSearch.
        """
        query = {
            "query": {
                "nested": {
                    "path": "embeddings",
                    "query": {
                        "knn": {
                            "embeddings.embedding": {
                                "vector": text_embedding,
                                "k": results_size,
                                "expand_nested_docs": True,
                                "rescore": True,
                            }
                        }
                    },
                    "inner_hits": {
                        "_source": False,
                        "fields": [
                            "embeddings.startSec",
                            "embeddings.endSec",
                            "embeddings.embeddingOption",
                            "embeddings.embedding",
                        ],
                        "size": 25,
                    },
                    "score_mode": "max",
                }
            },
            "size": results_size,
            "_source": {"excludes": ["embeddings.embedding"]},
        }

        try:
            search_results = opensearch_client.search(
                body=query, index=OPENSEARCH_INDEX_NAME
            )
            self.logger.debug(f"Search results: {search_results}")
            return search_results
        except Exception as err:
            self.logger.error(f"Error querying index: {err}")
            raise err

    def format_search_results_segments(self, raw_search_results: dict) -> dict:
        """Formats the raw search results for video segments from OpenSearch into a structured format.
        This function processes the raw search results returned by OpenSearch and converts them
        into a structured format that includes video segment metadata such as video name, title,
        summary, keywords, duration, S3 URI, keyframe URL, start and end times, embedding option,
        and score.
        Args:
            raw_search_results (dict): The raw search results from OpenSearch.
        Returns:
            dict: The formatted search results.
        """
        search_results = VideoSegmentSearchResults(results=[])

        for result in raw_search_results["hits"]["hits"]:
            source = result["_source"]
            segments = result["inner_hits"]["embeddings"]["hits"]["hits"]

            for segment in segments:
                search_result = VideoSegmentSearchResult(
                    videoName=source["videoName"],
                    title=source["title"],
                    summary=source["summary"],
                    keywords=source["keywords"],
                    durationSec=source["durationSec"],
                    s3URI=source["s3URI"],
                    keyframeURL=source["keyframeURL"],
                    score=result["_score"],
                    segmentId=segment["_nested"]["offset"],
                    startSec=segment["fields"]["embeddings.startSec"][0],
                    endSec=segment["fields"]["embeddings.endSec"][0],
                    embeddingOption=segment["fields"]["embeddings.embeddingOption"][0],
                    segmentScore=segment["_score"],
                )

                search_results.results.append(search_result)

        # search_results = search_results.sorted_by_segment_score()
        # print(search_results[0:2])
        return search_results.to_dict()

    @tool
    def semantic_search_for_video_segments(self, results_size: int = 6) -> dict:
        """Performs a semantic search for a list of unique video segments (2-10 second excerpts from the video) using the generated text embedding.
        This function uses the text embedding generated by the Marengo model to search for video segments in OpenSearch.
        The results are then formatted and returned.
        Args:
            text_embedding (list): The dense vector embedding (list of floats) to use for the search.
            results_size (int): The number of results to return.
        Returns:
            dict: The search results from OpenSearch.
        """
        if len(self.text_embedding) != 1_024:
            self.logger.error(
                "Text embedding does not have the correct dimensions. Cannot perform search."
            )
            return {}
        self.logger.info(f"Text embedding dimensions: {len(self.text_embedding)}")

        # Create an OpenSearch client
        opensearch_client = self.create_opensearch_client()

        # Perform the semantic search
        self.logger.info(
            f"Performing semantic search for video segments with embedding: {self.text_embedding[0:5]}..."
        )
        raw_search_results = self.semantic_search_segments(
            opensearch_client, self.text_embedding, results_size
        )

        # Format the search results
        search_results = self.format_search_results_segments(raw_search_results)
        self.logger.debug(f"Search results: {search_results}")
        return search_results
